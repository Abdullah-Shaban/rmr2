# What's new in 3.0.0

The main change in this release is a new serialization format. To simplify, it's faster than the old one. To be a bit more accurate, it is neck and neck wih the old one in many use cases, but trumps the old one where the old one was slow. The average test in `benchmarks.R` takes 16 seconds instead of 49, but, even more importantly, the longest goes from 533 to 51, which means rmr2 performance is a lot more predictable. Workarounds such as preferring simple vector types and hashing keys to a smaller domain should not be needed anymore, unless you need to eke out the last bit of performance. The main ingredient, besides an additional sprinkling of C++ magic to resolve differences between the R world and the Java/Hadoop one, is a new native format that splits data and metadata. With all new and shiny things there can be some surprises. The new format required a moderate amount of new code that is not battle-tested yet. Additionally it is *not compatible with previous versions*. rmr2 3.x will not read rmr2 2.x `native` format data. These are the main reason we decided to go for a major release even if this latest version is almost backward compatbile with the previous one. On the bright side, programs should keep working in most cases and require minor changes otherwise. Other noteworthy changes are: 

* The option `keyval.length` to `rmr.options` has been removed. It meant too many different things depending on what format was being used. It's now up to each format how it reads data and how big of a chunk it is willing to read or write in one step. Input format functions just take an open connection and return a key-value pair, output formats an open connection and a key-value pair to write out and what they do exactly is up to them. Existing IO formats have been upgraded to reflect this, for instance to create a CSV input format that reads 1729 lines of input at a time, enter `make.input.format("csv", nrows = 1729)`. 
* `dfs.exists`: an addition to the repertoire of backend-independent file manipulations, `dfs.exists` checks if a file exists.
* Updated hbase I/O formats to use base64 encoding of arguments, which fixes an incompatibility with the an upgraded Java HBase I/O format, courtesy of @khharut.
* Fixed reduce call counter: it did grossly overestimate the number of calls, should be an actual count now. This is important because it can help tackle performance issues. Too few reduce calls and you may bust memory limits, too many and it may slow down your program.
* New optional setting `HDFS_CMD`: rmr2 applies some heuristics to find the `hdfs` command. If they fail, the worst that can happen are some deprecation warnings that can be reproduced by entering `to.dfs(1)`, for instance. Many users seem to be quite upset by a deprecation warning, therefore it's now possible to provide the correct path to `hdfs` by setting the environment variable `HDFS_CMD` as in : `export HDFS_CMD=/the/correct/path/to/hdfs`. The exact location of this command depends on which hadoop distribution you are using and how it's installed, please refer to you hadoop distribution documentation. If you don't see the deprecation warning or you don't care, you don't need to do anything.
