<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>What&#39;s new in 2.1.0</title>

<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 12px;
   margin: 8px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 { 
   font-size:2.2em; 
}

h2 { 
   font-size:1.8em; 
}

h3 { 
   font-size:1.4em; 
}

h4 { 
   font-size:1.0em; 
}

h5 { 
   font-size:0.9em; 
}

h6 { 
   font-size:0.8em; 
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre {	
   margin-top: 0;
   max-width: 95%;
   border: 1px solid #ccc;
   white-space: pre-wrap;
}

pre code {
   display: block; padding: 0.5em;
}

code.r, code.cpp {
   background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * { 
      background: transparent !important; 
      color: black !important; 
      filter:none !important; 
      -ms-filter: none !important; 
   }

   body { 
      font-size:12pt; 
      max-width:100%; 
   }
       
   a, a:visited { 
      text-decoration: underline; 
   }

   hr { 
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote { 
      padding-right: 1em; 
      page-break-inside: avoid; 
   }

   tr, img { 
      page-break-inside: avoid; 
   }

   img { 
      max-width: 100% !important; 
   }

   @page :left { 
      margin: 15mm 20mm 15mm 10mm; 
   }
     
   @page :right { 
      margin: 15mm 10mm 15mm 20mm; 
   }

   p, h2, h3 { 
      orphans: 3; widows: 3; 
   }

   h2, h3 { 
      page-break-after: avoid; 
   }
}

</style>





</head>

<body>
<h1>What&#39;s new in 2.1.0</h1>

<h1>Speed and more speed</h1>

<p>Through a mix of behind-the-scenes changes and few new API extensions we have made rmr2 much faster. As a driving example we used a NLP taks, collocations, borrowed from the Cloudera blog. It&#39;s simple, we didn&#39;t make it up and we knew it hit some of the weaknesses of rmr2.0, specifically the case of small records and small groups in the reduce phase. The good news is that on that example we now are, in our tests, within striking distance of a native Java implementation, that is about 20% slower. The other side of the coin is that to reach that number we had to replace one line of R with 10 lines of C++ (in the example, the library already contains about 14% C++ code), but we think it&#39;s a good trade-off as compared to switching to C++ or Java altogether. We packed this little piece of C code in the library as the function <code>vsum</code> for your convenience, and it&#39;s just a fast version of <code>sapply(x, sum)</code> when <code>x</code> is a big list of relatively small vectors.</p>

<h2>More vectorization in the reduce phase</h2>

<p>The <code>mapreduce</code> function has an additional option, <code>vectorized.reduce</code>, which, when set to <code>TRUE</code>, makes <code>rmr2</code> call the user-supplied reduce function on not one but many reduce groups. The reduce groups are always complete (all the records for one key are processed in the same call). This is particularly helpful when dealing with small records and small reduce groups, such that vectorizing only on the values is not sufficient to achieve the desired performance. Check out the <a href="../pkg/examples/collocations.R">collocations example</a> to see it in action and <code>help(mapreduce)</code> has the gory details.</p>

<h2>Combine in memory</h2>

<p>Also a new option to <code>mapreduce</code>, <code>in.memory.combine</code> implements a super-early combine before data is serialized for the shuffle phase. It works best when the cardinality of the key set is small or they happen to be sorted in the input, or same keys are contiguos for some other reason. It can be used with or without a regular combiner. </p>

<h2>Helpful counters</h2>

<p>The goal of vectorization is to have an R program get more work done in the C layer than in the R interpreter. For a program using <code>rmr2</code> that means processing all your data with few, efficient map and reduce calls that take care of relatively big chunks of data. As an aid to optimizations of this type, we added two Hadoop counters, &ldquo;map calls&rdquo; and &ldquo;reduce calls&rdquo; that you can monitor inthe normal jobtracker web interface together with all other counters. If the number of one or the other is the same order of magnitude as the number of records in input and typical records are small, the answer is more vectorization.</p>

<h1>Other friendly features</h1>

<h2>Hbase input</h2>

<p>By popular demand, but still a little experimental, an input format that can read directly from hbase tables. Deserialization is configurable and a few common choices are provided. Please kick the tires and let us know how it&#39;s working for you or otherwise.</p>

<h2>Hadoop status and counters</h2>

<p>You can now set the task attempt status with <code>status</code> and increment counters with <code>increment.counter</code>. One use for these calls is to tell Hadoop that your map or reduce function is still alive. If your computation is very CPU bound and fails on time-outs for larger data sets but seems correct otherwise, updating the status or incrementing a counter every minute or so can solve the problem.</p>

<h2>Memory profiling</h2>

<p>We added the possiblity to memory profile the mapper and reducer, in addition to regular profiling.</p>

<h2>Concatenate keyval pairs</h2>

<p>The new function <code>c.keyval</code> is a convenience for people porting programs from rmr 1.3. A list of non-vectorized key-val pairs in rmr &lt; 2 can be converted into a rmr2-ready vectorized key-value pair with the help of this function.</p>

</body>

</html>

