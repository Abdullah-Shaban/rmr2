# What's new in 3.0.0

The main change in this release is a new serialization format. To simplify, it's faster than the old one. To be a bit more accurate, it may be a little slower on short jobs, measured in seconds, and on some sweet spots for the previous format. But it's faster when the old formats was inadequate, sometimes as much as 40X faster. The result is that the user doesn't need to think as hard when choosing keys and values, both their type and the number of distinct key. Techniques such as preferring simple vector types and hashing values to a smaller domain should not be needed as often anymore. Performance is a lot more predictable across a variety of use cases (see the file `tests/benchmark.R` in the package, under `pkg/` in the repo). The main ingredient, besides an additional sprinkling of C++ magic to resolve differences between the R world and the Java/Hadoop one, is a new format that splits data and metadata using Hadoop hidden files for the latter. This way we don't have to, for instance, represent column names in each record. It can get pretty expensive when a record contains a single float and the column name is very long. With all new and shiny things there can be some surprises. The new format required a lot of new code that is not battle-tested yet and is not compatible with previous versions. rmr2 3.x will not read rmr2 2.x `native` format data. These are the main reason we decided to go for a major release even if this latest version is almost backward compatbile with the previous one. On the bright side, programs should keep working in most cases and require minor changes otherwise. Other noteworthy changes

* The option `keyval.length` has been removed. It meant too many different things depending on what format was being used. It's now up to each format how it reads data and how big of a chunk it is willing to read or write in one step. Input format functions just take an open connection and return a key-value pair, output formats and open connection and a key-value pair to write out. Users can use closures to write more flexible I/O formats.
* `dfs.exists`: an addition to the repertoire of backend-independent file manipulations, `dfs.exists` checks if a file exists.
* Updated hbase I/O formats to use base64 encoding of arguments, which fixes an incompatibility with the an upgraded Java HBase I/O format.
* Fixed reduce call counter: it did grossly overestimate the number of calls, should be an actual count now. This is important because it can help tackle performance issues. Too few reduce calls and you may bust memory limits, too many and it may slow down your program.
