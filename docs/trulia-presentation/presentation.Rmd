`r read_chunk('../../pkg/tests/benchmarks.R')`
`r read_chunk('../../pkg/tests/basic-examples.R')`
`r read_chunk('../../pkg/tests/wordcount.R')`
`r read_chunk('../../pkg/tests/logistic-regression.R')`
`r read_chunk('../../pkg/tests/linear-least-squares.R')`
`r read_chunk('../../pkg/tests/kmeans.R')`
`r opts_chunk$set(echo=TRUE, eval=FALSE, cache=FALSE, tidy=FALSE)`


## Scalable Analytics in R with rmr
### Revolution Analytics
##### Antonio Piccolboni


# RHadoop

# <img src="../resources/hadoop-logo.gif">

## <img src = "../resources/Mapreduce.png">

<details>
operating system of the cloud --
focus is scalability --
different from HPC --
storage, fault tolerance built in, programming model
</details>

# <img src="../resources/R.png">

## <img src="https://r4stats.files.wordpress.com/2012/04/fig_10_cran.png" width=75%>

<details>[r4stats](http://r4stats.com/popularity) --
Google from niche to main language --
oreilly hot language --
SAS and mathematica compatibility
</details>

## <img src="../resources/revo-home.png" width=75%>

## <img src="../resources/rhadoop.png" width=50%>

<details>
hadoop brings horizontal scalability --
r sophisticated analytics --
combination could be powerful  
</details>

##

* rhdfs
* rhbase
* <em>rmr2</em>

<details>
why RHadoop for the R dev --
The data is in Hadoop --
The biggest cluster is Hadoop
</details>

##

```r
    library(rmr2)
  
    mapreduce(input, ...)
```

<details>
Just a library
Not a special run-time
Not a different language
Not a special purpose language
Incrementally port your code
Use all packages
</details>

##

```r
    sapply(data, function)

    mapreduce(big.data, map = function)
```

<details>
Very R-like, building on the functional characteristics of R
Upholds scope rules
</details>


## 
<table width=75% align=center>
<thead>
<th>Direct MR</th><th>Indirect MR</th>
</thead> 
<tr><td>&nbsp;</td></tr>
<tr>
<td></td><td><em>Hive, Pig</em></td>
</tr>
<tr><td>&nbsp;</td></tr>
<tr>
<td><strong>Rmr</strong>, Rhipe, Dumbo, Pydoop, Hadoopy</td><td>Cascalog, Scalding, Scrunch</td>
</tr>
<tr><td>&nbsp;</td></tr>
<tr> 
<td>Java, C++</td><td>Cascading, Crunch</td>
</tr>
</table>

<details>
Much simpler than writing Java
Not as simple as Hive, Pig at what they do, but more general, a real language
Great for prototyping, can transition to production -- optimize instead of rewriting! Lower risk, always executable.
</details>

##

```{.python style="font-size:12px"}
#!/usr/bin/python
import sys
from math import fabs
from org.apache.pig.scripting import Pig

filename = "student.txt"
k = 4
tolerance = 0.01

MAX_SCORE = 4
MIN_SCORE = 0
MAX_ITERATION = 100

# initial centroid, equally divide the space
initial_centroids = ""
last_centroids = [None] * k
for i in range(k):
  last_centroids[i] = MIN_SCORE + float(i)/k*(MAX_SCORE-MIN_SCORE)
  initial_centroids = initial_centroids + str(last_centroids[i])
  if i!=k-1:
    initial_centroids = initial_centroids + ":"

P = Pig.compile("""register udf.jar
          DEFINE find_centroid FindCentroid('$centroids');
          raw = load 'student.txt' as (name:chararray, age:int, gpa:double);
          centroided = foreach raw generate gpa, find_centroid(gpa) as centroid;
          grouped = group centroided by centroid;
          result = foreach grouped generate group, AVG(centroided.gpa);
          store result into 'output';
        """)

converged = False
iter_num = 0
while iter_num<MAX_ITERATION:
  Q = P.bind({'centroids':initial_centroids})
  results = Q.runSingle()
```


##

```{.python style="font-size:12px"}
  if results.isSuccessful() == "FAILED":
    raise "Pig job failed"
  iter = results.result("result").iterator()
  centroids = [None] * k
  distance_move = 0
  # get new centroid of this iteration, caculate the moving distance with last iteration
  for i in range(k):
    tuple = iter.next()
    centroids[i] = float(str(tuple.get(1)))
    distance_move = distance_move + fabs(last_centroids[i]-centroids[i])
  distance_move = distance_move / k;
  Pig.fs("rmr output")
  print("iteration " + str(iter_num))
  print("average distance moved: " + str(distance_move))
  if distance_move<tolerance:
    sys.stdout.write("k-means converged at centroids: [")
    sys.stdout.write(",".join(str(v) for v in centroids))
    sys.stdout.write("]\n")
    converged = True
    break
  last_centroids = centroids[:]
  initial_centroids = ""
  for i in range(k):
    initial_centroids = initial_centroids + str(last_centroids[i])
    if i!=k-1:
      initial_centroids = initial_centroids + ":"
  iter_num += 1

if not converged:
  print("not converge after " + str(iter_num) + " iterations")
  sys.stdout.write("last centroids: [")
  sys.stdout.write(",".join(str(v) for v in last_centroids))
  sys.stdout.write("]\n")
```

##

```{.java style="font-size:12px"}
import java.io.IOException;

import org.apache.pig.EvalFunc;
import org.apache.pig.data.Tuple;


public class FindCentroid extends EvalFunc<Double> {
  double[] centroids;
  public FindCentroid(String initialCentroid) {
    String[] centroidStrings = initialCentroid.split(":");
    centroids = new double[centroidStrings.length];
    for (int i=0;i<centroidStrings.length;i++)
      centroids[i] = Double.parseDouble(centroidStrings[i]);
  }
  @Override
  public Double exec(Tuple input) throws IOException {
    double min_distance = Double.MAX_VALUE;
    double closest_centroid = 0;
    for (double centroid : centroids) {
      double distance = Math.abs(centroid - (Double)input.get(0));
      if (distance < min_distance) {
        min_distance = distance;
        closest_centroid = centroid;
      }
    }
    return closest_centroid;
  }

}
```

# Read and Write

## 

<ul class="incremental" style="list-style: none" >
<li>
```{r write}
```
<li>
```{r read}
```
</ul>

# Identity

## 
```{r pass-through}
```

# Filter

## 
<ul class="incremental" style="list-style: none" >
<li>
```{r predicate }
```
<li> 
```{r filter }
```
</ul>

# Select

## 
<ul class="incremental" style="list-style: none" >
<li>
```{r select-input }
```
<li>
```{r select }
```
</ul>

# Sum

## 
<ul class="incremental" style="list-style: none" >
<li>
```{r bigsum-input}
```
<li>
```{r bigsum }
```
</ul>

# Group and Aggregate

## 

```{r group-aggregate-input}
```

## 

```{r group-aggregate-functions}
```

## 

```{r group-aggregate}
```

# Wordcount

##

<ul class="incremental" style="list-style: none" >
<li>
```{r wordcount-signature}
```
<li>
```{r wordcount-mapreduce}
```
</ul>

##

<ul class="incremental" style="list-style: none" >
<li>
```{r wordcount-map}
```
<li>
```{r wordcount-reduce}
```
</ul>

# K-means

##

```{r kmeans-dist.fun}
```
 
##
```{r kmeans.map}
```

##
```{r kmeans.reduce}
```

##
```{r kmeans-signature}
```
##
```{r kmeans-main-1}
```
##
```{r kmeans-main-2}
```
##

```{r kmeans-data}
```

##

```{r kmeans-run}
```  

<details>
Other features: easy composition of jobs, joins, local modes, combine 
</details>

##

#### Revolution Analytics
### rhadoop@revolutionanalytics.com
#### Antonio Piccolboni
### antonio@piccolboni.info